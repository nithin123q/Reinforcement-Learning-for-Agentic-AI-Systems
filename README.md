<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Reinforcement Learning for Agentic AI Systems - README</title>
</head>
<body>

<h1>Reinforcement Learning for Agentic AI Systems</h1>
<h2>Take-Home Final â€“ Northeastern University</h2>
<h3>Author: <b>Nithin Yash Menezes</b></h3>
<hr>

<h2>ğŸ¯ Project Overview</h2>
<p>
This project implements a <b>Reinforcement Learningâ€“powered Agentic AI Research Workflow System</b>.
The goal is to enable autonomous agents to learn through experience and optimize:
</p>

<ul>
    <li>Which information sources to query</li>
    <li>When to keep searching</li>
    <li>When to summarize and stop</li>
    <li>How to balance quality vs. computational cost</li>
</ul>

<p>The system uses:</p>
<ul>
    <li><b>Q-Learning</b> â€“ workflow decision-making</li>
    <li><b>UCB1 Bandit</b> â€“ best source selection</li>
    <li><b>Agent-based orchestration</b></li>
</ul>

<p>This creates an adaptive, self-improving research agent.</p>

<hr>

<h2>ğŸ§  Core Idea</h2>
<p>Traditional workflows are static. This system replaces them with a learning controller:</p>

<table border="1" cellpadding="8">
<tr><th>Component</th><th>Learns</th></tr>
<tr><td>Q-Learning</td><td>Best action (search_more / summarize / stop)</td></tr>
<tr><td>UCB1 Bandit</td><td>Best information source</td></tr>
<tr><td>Reward Function</td><td>Balance of quality and cost</td></tr>
</table>

<hr>

<h2>ğŸ—ï¸ System Architecture</h2>

<pre>
User Task Input
        â†“
RL Controller (Q-Learning + UCB Bandit)
        â†“                     â†“
 Search Agent â†’ Sources (Wiki / Blogs / Scholar / News)
        â†“
 Summarizer Agent
        â†“
 Reward Computation (Quality â€“ Cost)
        â†“
Q-Table & UCB Update
        â†º (feedback loop)
</pre>

<hr>

<h2>ğŸ“ Project Structure</h2>

<pre>
rl_agentic_final_latest/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                # RL training pipeline
â”‚   â”œâ”€â”€ main_fixed.py          # Baseline system (no learning)
â”‚   â”œâ”€â”€ agents.py              # Agents + RL Controller 
â”‚   â”œâ”€â”€ bandit.py              # UCB1 implementation
â”‚   â”œâ”€â”€ qlearning.py           # Q-Learning implementation
â”‚   â”œâ”€â”€ environment.py         # Reward model + environment logic
â”‚   â”œâ”€â”€ plot_learning_curve.py # Matplotlib learning plots
â”‚   â””â”€â”€ app.py                 # Streamlit interactive dashboard
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ rl_metrics.csv         # Logged episode rewards + searches
â”‚   â”œâ”€â”€ learning_curve_reward.png
â”‚   â””â”€â”€ learning_curve_searches.png
â”‚
â”œâ”€â”€ tasks.json                 # Task dataset (3 tasks)
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # This file
</pre>

<hr>

<h2>âš™ï¸ Installation</h2>

<h3>1ï¸âƒ£ Clone Repository</h3>
<pre>git clone &lt;your-repo-url&gt;
cd rl_agentic_final_latest</pre>

<h3>2ï¸âƒ£ Create Virtual Environment</h3>
<pre>python3 -m venv env
source env/bin/activate</pre>

<h3>3ï¸âƒ£ Install Dependencies</h3>
<pre>pip install -r requirements.txt</pre>

<hr>

<h2>â–¶ï¸ Running the System</h2>

<h3>Baseline (No RL)</h3>
<pre>python3 -m src.main_fixed</pre>

<h3>RL Agent (Q-Learning + UCB)</h3>
<pre>python3 -m src.main</pre>

<p>Produces:</p>
<ul>
    <li>Episode rewards</li>
    <li>Episode searches</li>
    <li>rl_metrics.csv</li>
</ul>

<hr>

<h2>ğŸ“Š Generate Learning Curves</h2>

<pre>python3 -m src.plot_learning_curve</pre>

<p>Creates:</p>
<ul>
    <li>learning_curve_reward.png</li>
    <li>learning_curve_searches.png</li>
</ul>

<hr>

<h2>ğŸ“º Streamlit Dashboard</h2>

<pre>streamlit run src/app.py</pre>

<p>The dashboard shows:</p>
<ul>
    <li>Reward learning curve</li>
    <li>Search-efficiency curve</li>
    <li>Episode metrics</li>
</ul>

<hr>

<h2>ğŸ§ª Experimental Setup</h2>

<table border="1" cellpadding="8">
<tr><th>Parameter</th><th>Value</th></tr>
<tr><td>Episodes</td><td>50</td></tr>
<tr><td>Learning Rate</td><td>0.1</td></tr>
<tr><td>Discount Factor</td><td>0.9</td></tr>
<tr><td>Max Searches</td><td>3</td></tr>
<tr><td>Exploration</td><td>UCB1 Bandit</td></tr>
<tr><td>Tasks</td><td>3 research questions</td></tr>
</table>

<hr>

<h2>ğŸ“ˆ Results Summary</h2>

<h3>1. Reward Learning Curve</h3>
<ul>
    <li>Rewards increase over time</li>
    <li>Shows convergence of Q-Learning</li>
</ul>

<h3>2. Search Efficiency</h3>
<ul>
    <li>Searches stabilize to ~1.5â€“2.3 per episode</li>
    <li>Indicates improved workflow</li>
</ul>

<h3>3. Baseline vs RL</h3>
<table border="1" cellpadding="8">
<tr><th>Metric</th><th>Baseline</th><th>RL System</th></tr>
<tr><td>Adaptation</td><td>No</td><td>Yes</td></tr>
<tr><td>Source Selection</td><td>Fixed</td><td>Adaptive</td></tr>
<tr><td>Workflow Decisions</td><td>Hardcoded</td><td>Learned</td></tr>
<tr><td>Performance</td><td>Medium</td><td><b>High</b></td></tr>
</table>

<hr>

<h2>ğŸ§  Mathematical Formulation</h2>

<h3>Q-Learning</h3>
<pre>
Q(s,a) â† Q(s,a) + Î± [r + Î³ max Q(sâ€™,aâ€™) â€“ Q(s,a)]
</pre>

<h3>UCB1</h3>
<pre>
UCB = avg reward + sqrt(2 ln N / n)
</pre>

<hr>

<h2>ğŸ› ï¸ Technologies Used</h2>
<ul>
    <li>Python</li>
    <li>Q-Learning</li>
    <li>UCB Bandit</li>
    <li>Streamlit</li>
    <li>Matplotlib</li>
    <li>JSON tasks</li>
</ul>

<hr>

<h2>ğŸ”® Future Work</h2>
<ul>
    <li>Add PPO / REINFORCE</li>
    <li>Multi-Agent RL</li>
    <li>Real web search APIs</li>
    <li>RLHF (Human Feedback)</li>
    <li>Transfer learning</li>
</ul>

<hr>

<h2>ğŸ‘¤ Author</h2>
<p><b>Nithin Yash Menezes</b><br>
MS in Information Systems<br>
Northeastern University (2024â€“2026)</p>

<hr>

<h2>ğŸ“˜ License</h2>
<p>This project is submitted for the <b>Take-Home Final: Reinforcement Learning for Agentic AI Systems</b> course.</p>

</body>
</html>
